# Architecture Diagram - FAANG-Ready Version

## ğŸ—ï¸ Complete System Architecture (With Failure Modes)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          EXTERNAL DATA SOURCE                               â”‚
â”‚                                                                             â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                    â”‚    CoinGecko API         â”‚                            â”‚
â”‚                    â”‚   (Public REST API)      â”‚                            â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                             â”‚                                               â”‚
â”‚                    Failure Mode: Rate limiting (50 req/min)                â”‚
â”‚                    Mitigation: Retry with exponential backoff              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ HTTP GET /simple/price
                              â”‚ Poll interval: 30s
                              â”‚ Serialization: JSON
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA INGESTION LAYER                                â”‚
â”‚                                                                             â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚              â”‚   Python Producer (crypto_price_producer.py)               â”‚
â”‚              â”‚                                        â”‚                    â”‚
â”‚              â”‚   â€¢ Fetches BTC/ETH every 30s          â”‚                    â”‚
â”‚              â”‚   â€¢ Enriches: timestamp, volume        â”‚                    â”‚
â”‚              â”‚   â€¢ Serialization: JSON (not Avro)    â”‚                    â”‚
â”‚              â”‚   â€¢ Error handling: 3 retries, 5s backoff                  â”‚
â”‚              â”‚                                        â”‚                    â”‚
â”‚              â”‚   Monitoring: Stdout logs only         â”‚                    â”‚
â”‚              â”‚   Scaling: Single instance (bottleneck at ~100 symbols)    â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                           â”‚                                                 â”‚
â”‚                  Failure Mode: CoinGecko API down                          â”‚
â”‚                  Mitigation: Retry logic, graceful skip                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ kafka-python-ng
                            â”‚ Topic: crypto-prices (3 partitions)
                            â”‚ Partitioning: Key-based (symbol) for ordering
                            â”‚ Replication: factor=1 (DEV ONLY, should be 3 in PROD)
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        MESSAGE STREAMING LAYER                              â”‚
â”‚                                                                             â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚         â”‚       Apache Kafka 7.5.0                 â”‚                       â”‚
â”‚         â”‚                                          â”‚                       â”‚
â”‚         â”‚  Topics:                                 â”‚                       â”‚
â”‚         â”‚  â€¢ crypto-prices (3 partitions)          â”‚                       â”‚
â”‚         â”‚  â€¢ crypto-alerts (1 partition)           â”‚                       â”‚
â”‚         â”‚                                          â”‚                       â”‚
â”‚         â”‚  Retention: 7 days / 1GB per partition   â”‚                       â”‚
â”‚         â”‚  Replication: 1 (DEV) / 3 (PROD)        â”‚                       â”‚
â”‚         â”‚                                          â”‚                       â”‚
â”‚         â”‚  Managed by: Zookeeper 7.5.0             â”‚                       â”‚
â”‚         â”‚  Persistence: Docker volumes (survives restarts)                â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                        â”‚                                                    â”‚
â”‚               Failure Mode: Broker crash                                   â”‚
â”‚               Mitigation: Kafka auto-recovery with persistent volumes      â”‚
â”‚                          Consumer offset tracking prevents data loss        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Flink Kafka Connector
                         â”‚ Consumer Group: flink-crypto-analyzer
                         â”‚ Offset strategy: Latest (not earliest)
                         â”‚ Deserialize: JSON â†’ PriceUpdate POJO
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       STREAM PROCESSING LAYER                               â”‚
â”‚                                                                             â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚         â”‚   Apache Flink 1.18 (Java 21)            â”‚                       â”‚
â”‚         â”‚   JobManager (1) + TaskManager (1)       â”‚                       â”‚
â”‚         â”‚   Parallelism: 1 (single instance)       â”‚                       â”‚
â”‚         â”‚                                          â”‚                       â”‚
â”‚         â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚                       â”‚
â”‚         â”‚   â”‚ CryptoPriceAggregator Job      â”‚     â”‚                       â”‚
â”‚         â”‚   â”‚                                â”‚     â”‚                       â”‚
â”‚         â”‚   â”‚ Event-time processing:         â”‚     â”‚                       â”‚
â”‚         â”‚   â”‚ â€¢ Watermarks: 10s out-of-order â”‚     â”‚                       â”‚
â”‚         â”‚   â”‚ â€¢ Tumbling windows: 1m, 5m, 15mâ”‚     â”‚                       â”‚
â”‚         â”‚   â”‚                                â”‚     â”‚                       â”‚
â”‚         â”‚   â”‚ Operations:                    â”‚     â”‚                       â”‚
â”‚         â”‚   â”‚ â€¢ OHLC aggregation (custom)    â”‚     â”‚                       â”‚
â”‚         â”‚   â”‚ â€¢ Anomaly detection (stateful) â”‚     â”‚                       â”‚
â”‚         â”‚   â”‚ â€¢ Dual sink (parallel writes)  â”‚     â”‚                       â”‚
â”‚         â”‚   â”‚                                â”‚     â”‚                       â”‚
â”‚         â”‚   â”‚ State: RocksDB (disk-based)    â”‚     â”‚                       â”‚
â”‚         â”‚   â”‚ Checkpoints: Every 60s         â”‚     â”‚                       â”‚
â”‚         â”‚   â”‚ Semantics: Exactly-once        â”‚     â”‚                       â”‚
â”‚         â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚                       â”‚
â”‚         â”‚                                          â”‚                       â”‚
â”‚         â”‚   Monitoring: Flink Web UI (http://localhost:8082)              â”‚
â”‚         â”‚   Metrics: Records in/out, backpressure, checkpoint duration     â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                        â”‚                                                    â”‚
â”‚               Failure Mode: TaskManager crash                              â”‚
â”‚               Mitigation: Restart from last checkpoint (60s max data loss) â”‚
â”‚                          State persisted in RocksDB                        â”‚
â”‚                                                                             â”‚
â”‚               Scaling: Add TaskManagers, increase parallelism              â”‚
â”‚               Bottleneck: Currently single-threaded (parallelism=1)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Triple Sink Pattern
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â†“              â†“              â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Redis     â”‚ â”‚PostgreSQLâ”‚ â”‚ Kafka Topic  â”‚
   â”‚  (Cache)    â”‚ â”‚(Historical)  â”‚(crypto-alerts)â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚              â”‚              â”‚
          â”‚ Jedis        â”‚ JDBC         â”‚ Kafka
          â”‚ Connection   â”‚ Connection   â”‚ Producer
          â”‚ Pool         â”‚ Pool         â”‚
          â”‚ (10 conns)   â”‚ (10 conns)   â”‚
          â”‚              â”‚              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         â”‚   STORAGE LAYER            â”‚                                     â”‚
â”‚         â”‚                            â”‚                                     â”‚
â”‚    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚    â”‚  Redis 7      â”‚          â”‚  PostgreSQL 15    â”‚                       â”‚
â”‚    â”‚               â”‚          â”‚  + TimescaleDB    â”‚                       â”‚
â”‚    â”‚  Data Model:  â”‚          â”‚                   â”‚                       â”‚
â”‚    â”‚  String keys  â”‚          â”‚  Tables:          â”‚                       â”‚
â”‚    â”‚  JSON values  â”‚          â”‚  â€¢ price_aggregates_1m                    â”‚
â”‚    â”‚               â”‚          â”‚    (hypertable, 1-day chunks)             â”‚
â”‚    â”‚  Keys:        â”‚          â”‚  â€¢ price_alerts   â”‚                       â”‚
â”‚    â”‚  crypto:{SYM}:â”‚          â”‚  â€¢ cryptocurrenciesâ”‚                       â”‚
â”‚    â”‚   latest      â”‚          â”‚                   â”‚                       â”‚
â”‚    â”‚               â”‚          â”‚  Indexes:         â”‚                       â”‚
â”‚    â”‚  TTL: 45s     â”‚          â”‚  â€¢ (crypto_id, window_start)              â”‚
â”‚    â”‚  (1.5x update)â”‚          â”‚    for range scansâ”‚                       â”‚
â”‚    â”‚               â”‚          â”‚                   â”‚                       â”‚
â”‚    â”‚  Pub/Sub:     â”‚          â”‚  Retention: Unlimited                     â”‚
â”‚    â”‚  Channel:     â”‚          â”‚  Backups: pg_dump nightly (if deployed)   â”‚
â”‚    â”‚  crypto:updates          â”‚                   â”‚                       â”‚
â”‚    â”‚  (PUBLISH on  â”‚          â”‚  UPSERT logic:    â”‚                       â”‚
â”‚    â”‚   cache write)â”‚          â”‚  ON CONFLICT (crypto_id, window_start)    â”‚
â”‚    â”‚               â”‚          â”‚  DO UPDATE SET... â”‚                       â”‚
â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚         â”‚                            â”‚                                     â”‚
â”‚    Failure: Redis down               Failure: PostgreSQL down              â”‚
â”‚    Impact: Latest prices fail        Impact: Historical queries fail      â”‚
â”‚    Mitigation: API returns 503       Mitigation: API returns 503           â”‚
â”‚               Flink continues        Flink buffers (checkpoint state)      â”‚
â”‚               writing to PostgreSQL  writes resume on recovery             â”‚
â”‚                                                                             â”‚
â”‚    Scaling: Redis Cluster with      Scaling: Read replicas + connection   â”‚
â”‚             sharding by symbol      pooling (currently 10 conns max)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                      â”‚
              â”‚ Redis client         â”‚ psycopg2
              â”‚ GET/SUBSCRIBE        â”‚ SQL queries
              â†“                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            API LAYER                                        â”‚
â”‚                                                                             â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚              â”‚   FastAPI Backend (Python 3.11)        â”‚                    â”‚
â”‚              â”‚   Uvicorn ASGI server                  â”‚                    â”‚
â”‚              â”‚                                        â”‚                    â”‚
â”‚              â”‚   REST Endpoints:                      â”‚                    â”‚
â”‚              â”‚   â€¢ GET /api/v1/latest/{symbol}        â”‚                    â”‚
â”‚              â”‚     Cache: Redis, Latency: <10ms       â”‚                    â”‚
â”‚              â”‚   â€¢ GET /api/v1/historical/{symbol}    â”‚                    â”‚
â”‚              â”‚     Storage: PostgreSQL, Latency: <200ms                    â”‚
â”‚              â”‚   â€¢ GET /api/v1/alerts/{symbol}        â”‚                    â”‚
â”‚              â”‚   â€¢ GET /health (Redis + PostgreSQL checks)                 â”‚
â”‚              â”‚                                        â”‚                    â”‚
â”‚              â”‚   WebSocket:                           â”‚                    â”‚
â”‚              â”‚   â€¢ WS /ws/prices/{symbol}             â”‚                    â”‚
â”‚              â”‚   â€¢ Mode: Event-driven (Redis Pub/Sub) â”‚                    â”‚
â”‚              â”‚   â€¢ Latency: <100ms (measured)         â”‚                    â”‚
â”‚              â”‚   â€¢ Clients: Tested up to 25, can handle ~50               â”‚
â”‚              â”‚                                        â”‚                    â”‚
â”‚              â”‚   Connection Pooling:                  â”‚                    â”‚
â”‚              â”‚   â€¢ Redis: 10 max connections          â”‚                    â”‚
â”‚              â”‚   â€¢ PostgreSQL: 10 max connections     â”‚                    â”‚
â”‚              â”‚                                        â”‚                    â”‚
â”‚              â”‚   Middleware:                          â”‚                    â”‚
â”‚              â”‚   â€¢ Performance logging (X-Request-ID) â”‚                    â”‚
â”‚              â”‚   â€¢ CORS (configured for localhost)    â”‚                    â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                           â”‚                                                 â”‚
â”‚                  Failure Mode: API instance crash                          â”‚
â”‚                  Mitigation: Manual restart (no load balancer in dev)      â”‚
â”‚                             Should use: Kubernetes with multiple replicas  â”‚
â”‚                                                                             â”‚
â”‚                  Scaling: Horizontal (add API instances behind LB)         â”‚
â”‚                  Bottleneck: Single instance, ~50 WebSocket clients max    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ HTTP REST / WebSocket
                            â”‚ Port: 8000 (localhost only)
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        VISUALIZATION LAYER                                  â”‚
â”‚                                                                             â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚              â”‚   Streamlit Dashboard (Python)         â”‚                    â”‚
â”‚              â”‚   Port: 8501                           â”‚                    â”‚
â”‚              â”‚                                        â”‚                    â”‚
â”‚              â”‚   Components:                          â”‚                    â”‚
â”‚              â”‚   â€¢ Live price cards (Streamlit metrics)                    â”‚
â”‚              â”‚   â€¢ Candlestick charts (Plotly)        â”‚                    â”‚
â”‚              â”‚   â€¢ Volume correlation (Plotly subplots)â”‚                   â”‚
â”‚              â”‚   â€¢ Moving averages (Pandas rolling)   â”‚                    â”‚
â”‚              â”‚   â€¢ Alert panel (Streamlit containers) â”‚                    â”‚
â”‚              â”‚                                        â”‚                    â”‚
â”‚              â”‚   Data fetching:                       â”‚                    â”‚
â”‚              â”‚   â€¢ Auto-refresh: 2s (Streamlit component)                  â”‚
â”‚              â”‚   â€¢ HTTP client: requests library      â”‚                    â”‚
â”‚              â”‚   â€¢ Retry: 3 attempts with backoff     â”‚                    â”‚
â”‚              â”‚                                        â”‚                    â”‚
â”‚              â”‚   Rendering: Server-side (Streamlit),  â”‚                    â”‚
â”‚              â”‚             Client-side (Plotly JS)    â”‚                    â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                             â”‚
â”‚                  Failure Mode: API unavailable                             â”‚
â”‚                  Mitigation: Shows error message with recovery steps       â”‚
â”‚                             Graceful degradation (partial features work)   â”‚
â”‚                                                                             â”‚
â”‚                  Scaling: Streamlit Cloud (managed),                       â”‚
â”‚                          or Docker behind nginx                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Data Flow with Serialization Details

### 1. Ingestion (Every 30 seconds)
```
HTTP Response (JSON)
â†“ Python dict
kafka-python Producer
â†“ JSON.dumps() â†’ bytes
Kafka (persistent log)
```

**Why JSON not Avro?**
- Simplicity for learning project
- Human-readable logs for debugging
- **Trade-off:** 3-5x larger messages, no schema evolution support

---

### 2. Stream Processing (Continuous)
```
Kafka Consumer
â†“ JSON.parse() â†’ Java POJO (PriceUpdate)
Flink Watermark Assignment (event-time from "timestamp" field)
â†“ 10-second buffer for late arrivals
Tumbling Windows (1m, 5m, 15m)
â†“ Aggregate function (OHLCAggregator)
Window Trigger
â†“ Output: OHLCCandle POJO
Dual Sink
â”œâ”€â†’ Redis: SETEX + PUBLISH (JSON string)
â”œâ”€â†’ PostgreSQL: JDBC batch insert (100 records/batch or 5s interval)
â””â”€â†’ Kafka: Alerts topic (JSON)
```

**Exactly-Once Guarantees:**
- Flink checkpoints every 60s
- Kafka transactions (idempotent producer)
- PostgreSQL UPSERT (ON CONFLICT)

---

### 3. API Serving (On-demand)
```
Client HTTP Request
â†“ FastAPI routing
â”œâ”€â†’ /latest â†’ Jedis.get() â†’ <1ms â†’ JSON response
â”œâ”€â†’ /historical â†’ psycopg2.execute() â†’ 50-200ms â†’ JSON response  
â””â”€â†’ /ws â†’ Redis Pub/Sub listener â†’ Async broadcast â†’ <100ms push
```

**Connection Pooling:**
- Redis: JedisPool (10 max)
- PostgreSQL: SimpleConnectionPool (1-10 connections)

---

## ğŸš¨ Failure Modes & Recovery

### Scenario 1: Redis Crashes
**Impact:**
- âŒ `/latest` endpoints return 503
- âŒ WebSocket stops receiving updates
- âœ… Flink continues writing to PostgreSQL
- âœ… Historical data queries still work

**Recovery:**
- Docker restart policy: `on-failure`
- Redis recovers in ~5 seconds
- Flink backfills cache with latest window

**Lesson:** Cache failures don't affect source of truth (PostgreSQL).

---

### Scenario 2: PostgreSQL Crashes
**Impact:**
- âŒ `/historical` endpoints return 503
- âŒ Flink JDBC sink fails, retries 3x
- âœ… `/latest` endpoints still work (Redis)
- âš ï¸ Flink job may fail if retries exhausted

**Recovery:**
- Docker restart policy: `on-failure`
- PostgreSQL recovers from persistent volume
- Flink restarts from last checkpoint (max 60s data loss)

**Improvement Needed:** Circuit breaker to skip PostgreSQL sink if down.

---

### Scenario 3: Flink TaskManager Crashes
**Impact:**
- âš ï¸ Processing stops for ~10-30 seconds
- âŒ No new OHLC windows during restart
- âœ… Kafka retains messages (7-day retention)

**Recovery:**
- Flink JobManager detects failure
- Restarts from last checkpoint (RocksDB state)
- Replays Kafka messages from checkpoint offset
- No data loss (exactly-once semantics)

**Current Limitation:** Single TaskManager (no HA).

---

### Scenario 4: CoinGecko API Rate Limit
**Impact:**
- âš ï¸ Producer fails to fetch prices
- âŒ No new messages to Kafka

**Recovery:**
- Producer retry logic: 3 attempts with exponential backoff
- Skips failed fetch, continues next interval
- Logs error for monitoring

**Gap:** No dead letter queue, no alerting.

---

## ğŸ“ˆ Scalability Analysis

### Current Capacity
**Single Instance Limits:**
- Producer: ~2 symbols * 2 fetches/min = **4 requests/min to CoinGecko**
  - Limit: 50 req/min (CoinGecko free tier)
  - **Can scale to ~25 symbols** before hitting rate limit

- Flink (parallelism=1): **~20-30 msgs/sec** (measured: 0.1 msgs/sec actual)
  - Bottleneck: Single-threaded processing
  - **Can scale to ~1000 symbols** with parallelism=10

- API (single instance): **~50 WebSocket clients** (tested: 25)
  - Bottleneck: CPU for JSON serialization + broadcasting
  - **Can scale to 500+** with multiple instances behind load balancer

- Dashboard: **1 user** (Streamlit is single-session by default)
  - **Can scale to unlimited users** with Streamlit Cloud

### How to Scale 10x (to 20 symbols, 500 clients)

1. **Producer:**
   - No change needed (20 symbols = 40 req/min, under limit)

2. **Kafka:**
   - Increase partitions: 3 â†’ 20 (one per symbol)
   - Add broker: 1 â†’ 3 (replication factor=3)

3. **Flink:**
   - Increase parallelism: 1 â†’ 10
   - Add TaskManagers: 1 â†’ 5 (2 slots each)

4. **Redis:**
   - No change needed (write load is constant)

5. **PostgreSQL:**
   - Add read replica for query load
   - Connection pool: 10 â†’ 50

6. **API:**
   - Deploy: 1 â†’ 5 instances behind nginx load balancer
   - Sticky sessions for WebSocket (client â†’ same API instance)

7. **Dashboard:**
   - Deploy to Streamlit Cloud (auto-scales)

**Estimated Cost (AWS):**
- Current (dev): $0/month (local Docker)
- 10x scale: ~$800/month (MSK, ECS, RDS, ElastiCache)

---

## ğŸ¯ Data Serialization Trade-offs

### Current: JSON Everywhere
**Why:**
- Easy debugging (human-readable)
- No schema registry needed
- Python/Java interop is trivial

**Downsides:**
- 3-5x larger than Avro (~500 bytes vs ~150 bytes per message)
- No schema evolution support
- Slower serialization

### Alternative: Apache Avro
**Benefits:**
- Compact binary format
- Schema evolution (forward/backward compatibility)
- 60-70% size reduction

**Complexity:**
- Need Confluent Schema Registry
- More complex Java/Python setup
- Harder debugging

**Decision:** JSON is fine for 2 symbols, 6 msgs/min. At 1000 msgs/sec, switch to Avro.

---

## ğŸ” Monitoring & Observability (MISSING - Future Work)

### What Should Be Added

**Metrics Collection:**
```
Flink â†’ Prometheus (JMX exporter)
API â†’ Prometheus (Python client)
Redis â†’ Redis Exporter
PostgreSQL â†’ Postgres Exporter
         â†“
    Prometheus Server
         â†“
    Grafana Dashboards
```

**Dashboards to Create:**
1. **Producer Health:** Success rate, API latency, messages/sec
2. **Flink Metrics:** Records in/out, backpressure, checkpoint duration
3. **API Performance:** Request rate, p50/p95/p99 latency, error rate
4. **Storage Health:** Redis hit rate, PostgreSQL query time, disk usage

**Alerting Rules:**
- Producer failure (no messages for 2 minutes)
- Flink job restart (checkpoint failure)
- API p99 latency >500ms
- Redis memory >80%
- PostgreSQL connection pool exhausted

---

## ğŸ“ Architecture Decisions Interview Questions

Be prepared to answer:

**Q: Why 3 Kafka partitions?**
**A:** Supports 3 parallel consumers for scalability. Chose 3 (not 1 or 10) because:
- More than symbols (2) for future growth
- Odd number helps with rebalancing
- 3 TaskManagers would utilize fully

**Q: Why 1-minute windows, not 5 seconds?**
**A:** Trade-off between granularity and statistical significance. 
- 5-second windows = 6 price samples (noisy, frequent spikes)
- 1-minute windows = 2 samples (more stable, fewer false alerts)
- Financial data typically uses 1-minute or 1-hour candles

**Q: Why persist Flink state to RocksDB, not in-memory?**
**A:** Recovery from crashes. In-memory state is lost on TaskManager failure. RocksDB persists to disk, survives restarts. Trade-off: Slower state access (disk I/O), but necessary for production.

**Q: Why single Flink JobManager (single point of failure)?**
**A:** Development simplicity. Production should use Flink HA with ZooKeeper or Kubernetes. Current SPOF is acceptable for learning project, unacceptable for production.

---

## ğŸ¨ Creating the Visual Diagram

### Use Draw.io (30 minutes, NON-NEGOTIABLE)

**Required Elements:**
1. All components as boxes
2. Data flow arrows with labels (HTTP, Kafka, JDBC, Redis GET)
3. Failure mode annotations (red text boxes)
4. Latency annotations (e.g., "<10ms" near Redis)
5. Scaling notes (e.g., "Currently parallelism=1, can scale to 10")

**Color Coding:**
- Blue: External (CoinGecko)
- Green: Processing (Producer, Flink)
- Orange: Storage (Redis, PostgreSQL, Kafka)
- Purple: Serving (API, Dashboard)
- Red: Failure annotations

**Export as PNG:** 1920x1080 or higher, <2MB file size

**This diagram goes in your README and is shown in interviews.**

---

## âœ… Critical Path to v1.0.0

**Do ONLY this:**

1. **Create architecture diagram in Draw.io** (30 min)
   - Show failure modes
   - Show scaling paths
   - Save as: `docs/screenshots/architecture-diagram.png`

2. **Capture 3 screenshots** (15 min)
   - dashboard-candlestick-ma.png
   - dashboard-overview.png
   - architecture-diagram.png (export from Draw.io)

3. **Update README** (already done above - just add images)

4. **Git commit + merge + tag v1.0.0** (15 min)

**Total: 60 minutes to FAANG-ready**

---

**Status:** Ready to execute  
**Next:** Create Draw.io diagram with failure modes, commit, TAG v1.0.0